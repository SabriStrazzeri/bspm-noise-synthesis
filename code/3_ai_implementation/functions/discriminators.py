from keras.models import Sequential
from keras.layers import Dense, Flatten, LeakyReLU, Dropout, Conv2D
import tensorflow as tf

def disc_model_critic(rows, cols):
    """
    Constructs a discriminator (critic) model for a Wasserstein Generative Adversarial Network 
    with Gradient Penalty (WGAN-GP).

    This model is designed to distinguish between real and fake images/data samples.
    It takes an input image of a specified `rows` and `cols` dimension with a single channel,
    applies convolutional layers, Leaky ReLU activations, and Dropout for regularization,
    then flattens the output and uses a final Dense layer with no activation to output
    a single scalar value (the critic's score).

    Args:
        rows (int): The number of rows (height) of the input images/data samples.
        cols (int): The number of columns (width) of the input images/data samples.

    Returns:
        keras.Model: A Keras Sequential model representing the discriminator (critic).
    """

    model = Sequential()

    # First convolutional block: Downsamples and extracts features
    model.add(Conv2D(8, kernel_size=5, strides=2, input_shape=(rows, cols, 1), padding="same"))
    model.add(LeakyReLU(alpha=0.2)) # Non-linear activation for GANs
    model.add(Dropout(0.3))  # Regularization to prevent overfitting

    # Second convolutional block: Further downsamples and extracts features
    model.add(Conv2D(16, kernel_size=5, strides=2, padding="same"))
    model.add(LeakyReLU(alpha=0.2)) # Non-linear activation for GANs
    model.add(Dropout(0.3)) # Regularization to prevent overfitting

    # Flatten the output from the convolutional layers to feed into a Dense layer
    model.add(Flatten())
    
    # Final Dense layer: Outputs a single scalar value. No activation is used
    # because for a WGAN critic, this output represents a raw "score" or "criticism"
    # rather than a probability
    model.add(Dense(1)) # No activation function for critic output

    # Prints a summary of the model's architecture, including layer types,
    # output shapes, and number of parameters
    print(f"######### Critic (discriminator) Summary #########")
    model.summary()
    

    return model



def gradient_penalty(critic, real_imgs, fake_imgs, lambda_gp):
    """
    Calculates the Gradient Penalty (GP) for a WGAN-GP.

    The gradient penalty is a regularization term added to the critic's loss
    in WGAN-GP to enforce the 1-Lipschitz constraint. It penalizes the critic
    if the norm of its gradients with respect to interpolated samples deviates from 1.

    Args:
        critic (tf.keras.Model): The discriminator (critic) model.
        real_imgs (tf.Tensor): A batch of real images/data samples.
        fake_imgs (tf.Tensor): A batch of fake images/data samples generated by the generator.
        lambda_gp (float): The regularization coefficient for the gradient penalty.

    Returns:
        tf.Tensor: The calculated gradient penalty, scaled by `lambda_gp`.
    """

    # Generate random interpolation weights (alpha) for mixing real and fake images
    # Alpha is sampled from a uniform distribution between 0 and 1
    alpha = tf.random.uniform(
        [tf.shape(real_imgs)[0], 1, 1, 1], 0., 1.,
        dtype=tf.float32 # Explicitly float32
    )
    
    # Calculate (1 - alpha), explicitly ensuring it's float32
    one_minus_alpha = tf.cast(1.0 - alpha, tf.float32)

    # Create interpolated samples by linearly combining real and fake images
    interpolated_imgs = real_imgs * alpha + fake_imgs * one_minus_alpha # Using explicit one_minus_alpha

    # Use a GradientTape to compute gradients of the critic's output
    # with respect to the interpolated samples
    with tf.GradientTape() as gp_tape:
        gp_tape.watch(interpolated_imgs)
        interpolated_pred = critic(interpolated_imgs, training=True)

    # Compute the gradients.
    grads = gp_tape.gradient(interpolated_pred, interpolated_imgs)

    # Calculate the L2 norm (magnitude) of the gradients for each interpolated sample
    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))

    # Calculate the gradient penalty term: (norm - 1.0)^2
    # This penalizes deviations of the gradient norm from 1
    gp = tf.reduce_mean((norm - 1.0)**2)
    

    return gp * lambda_gp